{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datacamp CapGemini\n",
    "#### Group 7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aims : preprocessing and topic modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data = pd.read_csv('../data/data_scraping_V2.csv', engine='python')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove ill-formated lines\n",
    "raw_data = raw_data[raw_data.text.notnull()]\n",
    "# Remove empty lines\n",
    "mask = raw_data[\"text\"].map(lambda x: x.strip() == '')\n",
    "raw_data = raw_data.loc[~mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data.source.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove youtube and twitter\n",
    "excluded_sources = [\"youtube\", \"twitter\"]\n",
    "data = raw_data[raw_data[\"source\"].map(lambda x: x not in excluded_sources)].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### First steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove ponctuation\n",
    "matrix = str.maketrans(\",\\\"_;\", \"    \", \"'â€™.()/-?!|:><&[]*=@%^\")\n",
    "data[\"text\"] = data[\"text\"].transform(lambda x: x.translate(matrix))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove bad characters\n",
    "data[\"text\"] = data[\"text\"].transform(lambda text: ''.join([x for x in text if ord(x)<128]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regex-based processing\n",
    "import regex\n",
    "\n",
    "# Remove hashtags\n",
    "data[\"text\"] = data[\"text\"].map(lambda x: regex.sub('#[a-zA-Z0-9-]*', '', x))\n",
    "# Remove number only strings\n",
    "numbers = regex.compile('^[0-9 ]+$')\n",
    "mask = data[\"text\"].map(lambda x: not numbers.match(x))\n",
    "data = data[mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove empty lines\n",
    "mask = data[\"text\"].map(lambda x: x.strip() == '')\n",
    "data = data.loc[~mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detect language\n",
    "import langdetect\n",
    "def detect_lang(x):\n",
    "    try: \n",
    "        return langdetect.detect(x)\n",
    "    except Exception as e:\n",
    "        return None\n",
    "\n",
    "data[\"lang\"] = data[\"text\"].progress_map(detect_lang)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove reviews for which lang detect failed\n",
    "data = data[~data[\"lang\"].isnull()]\n",
    "# Only keep english comments\n",
    "data = data[data[\"lang\"] == \"en\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "tweet = TweetTokenizer(preserve_case=False, strip_handles=True, reduce_len=True)\n",
    "stopwords = set(stopwords.words('english'))\n",
    "\n",
    "# Tokenize\n",
    "data[\"text\"] = data[\"text\"].transform(tweet.tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove stopwords\n",
    "data[\"text\"] = data[\"text\"].transform(lambda x: [token for token in x if not token in stopwords])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove tokens with only numbers\n",
    "numbers = regex.compile('^[0-9]{3,}$')\n",
    "data[\"text\"] = data[\"text\"].map(lambda x: [token for token in x if not numbers.match(token)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove short lines\n",
    "data = data[data[\"text\"].apply(lambda x: len(x) > 5)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Checkpoint\n",
    "data.groupby(\"source\").head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stemming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### First attempt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.snowball import SnowballStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = SnowballStemmer('english')\n",
    "\n",
    "def stemming(tokens):\n",
    "    excluded = set(['iphone'])\n",
    "    return [stemmer.stem(token) if token not in excluded else token for token in tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"review_text\"].transform(stemming)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemming(tokens):\n",
    "    return [lemmatizer.lemmatize(token, pos=\"v\") for token in tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"text\"] = data[\"text\"].transform(lemming)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.groupby(\"source\").head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Token cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"text\"].sample(50).map(lambda x: print(\" \".join(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom tokens\n",
    "def custom_lemming(tokens):\n",
    "    processed = []\n",
    "    extend = processed.extend\n",
    "    length = len(tokens)\n",
    "    \n",
    "    for i, token in enumerate(tokens):\n",
    "        # iPhones\n",
    "        if token == \"x\" or token == \"10\":\n",
    "            result = [\"10\"]\n",
    "            if i>0 and tokens[i-1] != \"iphone\":\n",
    "                result.insert(0, \"iphone\")\n",
    "            extend(result)\n",
    "            continue\n",
    "        if token in [\"6\", \"7\", \"8\"]:\n",
    "            result = [token]\n",
    "            if i>0 and tokens[i-1] != \"iphone\":\n",
    "                result.insert(0, \"iphone\")\n",
    "            extend(result)\n",
    "            continue\n",
    "        if token == \"+\":\n",
    "            extend([\"plus\"])\n",
    "        extend([token])\n",
    "        \n",
    "        # Samsung\n",
    "    return processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"text\"] = data[\"text\"].transform(custom_lemming)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get bigrams\n",
    "from gensim.models.phrases import Phrases, Phraser\n",
    "\n",
    "phrases = Phrases(data[\"text\"].values.tolist())\n",
    "bigram = Phraser(phrases)\n",
    "\n",
    "data[\"bigrams\"] = list(bigram[data[\"text\"].values.tolist()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data[\"bigrams\"].sample(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tfidf = TfidfVectorizer(max_features=70, norm='l2', min_df=2, max_df=0.8, ngram_range=(1, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_matrix = tfidf.fit_transform(data[\"bigrams\"].transform(lambda x: ' '.join(x)).tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Features : {}\".format(\", \".join(tfidf.get_feature_names())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(tfidf_matrix.todense(), columns=tfidf.get_feature_names()).replace(0, '')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NMF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import NMF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nmf = NMF(n_components=10, alpha=.1, l1_ratio=.5).fit(tfidf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tdidf = T * H\n",
    "# H maps documents (articles) into new dimensions (in the case of NMF, we can interpret these as topics)\n",
    "# W maps words to new dimensions\n",
    "T = nmf.fit_transform(tfidf_matrix)\n",
    "W = nmf.components_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top = 10\n",
    "features = tfidf.get_feature_names()\n",
    "\n",
    "for i, dimension in enumerate(W):\n",
    "    print(\"Topic #{}\".format(i+1))\n",
    "    feature_indexes = dimension.argsort()[:-top:-1]\n",
    "    print(\"Words : {}\".format(\", \".join([features[i] for i in feature_indexes])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import models, corpora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create corpora dictionary\n",
    "tokens_dict = corpora.Dictionary(data[\"bigrams\"].values.tolist())\n",
    "print(tokens_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter extremes\n",
    "tokens_dict.filter_extremes(no_below=3, no_above=0.7)\n",
    "print(tokens_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create corpus\n",
    "corpus = [tokens_dict.doc2bow(review) for review in data[\"text\"].values.tolist()]\n",
    "print(corpus[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the LDA (computation time should be between 5 to 60 seconds)\n",
    "\n",
    "# choose the number of topics => to find a \"good\" number of topics, try multiple values and see which one is the best\n",
    "# optionally: input alpha and eta to influence how topics are distributed across documents, \n",
    "#  and how words are distributed across topics\n",
    "#  the syntax is the following\n",
    "#  alpha is a vector of size the number of documents, and eta's size is the number of words\n",
    "#  alpha = [0.01] * id2word_newspaper.num_docs for instance\n",
    "#  eta = [0.01] * len(id2word_newspaper.keys())\n",
    "\n",
    "num_topics = 40\n",
    "\n",
    "# Below without alpha nor eta\n",
    "%time lda_model = models.LdaModel(corpus, num_topics=num_topics, id2word=tokens_dict, passes=4)\n",
    "\n",
    "# Below with alpha and eta\n",
    "# %time lda_model = models.LdaModel(corpus, num_topics=num_topics, id2word=id2word_newspaper, passes=4, \n",
    "#                                   alpha=[0.01] * id2word_newspaper.num_docs, eta = [0.01] * len(id2word_newspaper.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_model.show_topics(num_topics=num_topics, num_words=8, formatted=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Graph of words\n",
    "\n",
    "Graph of words use words that are neighbors in sentences. <br>For instance, in the sentence \"Graph of words use words that are neighbors in sentences\", the table below describes the neighbors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| word_1 | word_2 |\n",
    "|-|-|\n",
    "| Graph | of |\n",
    "| of | words |\n",
    "| words | use |\n",
    "| use | words |\n",
    "| words | that |\n",
    "| that | are |\n",
    "| are | neighbors |\n",
    "| neighbors | in |\n",
    "| in | sentences |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll handle different things as well:\n",
    "- keeping only nouns\n",
    "- using words that are the 2nd neighbors (neighbor of neighbor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx # to analyse graphs in Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We aggregate data from ALL the comments (in our cleaned dataframe)\n",
    "# And take the words (tokens) that are nouns\n",
    "clean_text = df_try.noun_tokens.tolist()\n",
    "\n",
    "# The functions below will help us build the dataframe of words that are neighbors\n",
    "def clean_stop_words_in_dataframe(df, stop_words):\n",
    "    idx_1 = df.loc[df[df.columns[0]].isin(stop_words)].index\n",
    "    idx_2 = df.loc[df[df.columns[1]].isin(stop_words)].index\n",
    "    return df.loc[~(df.index.isin(idx_1.append(idx_2)))]\n",
    "\n",
    "def word_neighbors(dist):\n",
    "    return clean_stop_words_in_dataframe(\n",
    "        pd.concat([pd.DataFrame([clean_sentence[:-dist], clean_sentence[dist:]]).T for clean_sentence in clean_text]) \\\n",
    "        .rename(columns={0:'w0', 1:'w1'}).reset_index(drop=True), stop_words=STOPWORDS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "clean_text[0] # nouns of the first comment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# This creates a huge table of all the words that are neighbors and 2nd-order neighbors\n",
    "# For neighbors we use weight = 2, for 2nd-order neighbors we use weight = 1\n",
    "data_graph_of_words = word_neighbors(1).assign(weight=2).append(word_neighbors(2).assign(weight=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_graph_of_words.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We sum the weights for all combinations of neighbors\n",
    "data_graph_of_words = data_graph_of_words.groupby(['w0', 'w1']).weight.sum().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nx.__version__ is 2.1\n",
    "# If you have previous versions, the function might be nx.from_pandas_dataframe()\n",
    "graph_of_words = nx.from_pandas_edgelist(data_graph_of_words, source='w0', target='w1', edge_attr='weight', \n",
    "                                          create_using=nx.Graph())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# We select the words that are neighbors (and 2nd-order neighbors) of the word \"problem\"\n",
    "graph_of_words_center = nx.ego_graph(graph_of_words, n='problem', radius=1)\n",
    "print(graph_of_words_center.size())\n",
    "print(len(graph_of_words_center))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Which words are the most connected to \"problem\"?\n",
    "# Degree is the weight\n",
    "pd.DataFrame.from_dict([dict(graph_of_words_center.degree(graph_of_words_center.nodes, weight='weight'))]) \\\n",
    "    .T.rename(columns={0:'degree'}).reset_index().rename(columns={'index':'word'}).sort_values('degree', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Draw the graph as it is\n",
    "nx.draw(graph_of_words_center, node_size=20)\n",
    "# It doesn't give us a lot of information, except that many words connected to \"problem\" are connected together\n",
    "# (there's more than one line for each red dot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can use PageRank algorithm to see if some words are more connected to others\n",
    "pagerank = pd.DataFrame.from_dict([nx.pagerank(G=graph_of_words, alpha=0.99)]).T.rename(columns={0:'pagerank'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# It confirms what we had with LDA: \"phone\", \"screen\", \"iphone\"... are connected to too many words\n",
    "pagerank.sort_values('pagerank', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's group words into communities, and see if it makes sense in terms of topics\n",
    "# The code is taken from the link below\n",
    "# https://stackoverflow.com/questions/43541376/how-to-draw-communities-with-networkx\n",
    "def community_layout(g, partition):\n",
    "    \"\"\"\n",
    "    Compute the layout for a modular graph.\n",
    "\n",
    "\n",
    "    Arguments:\n",
    "    ----------\n",
    "    g -- networkx.Graph or networkx.DiGraph instance\n",
    "        graph to plot\n",
    "\n",
    "    partition -- dict mapping int node -> int community\n",
    "        graph partitions\n",
    "\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    pos -- dict mapping int node -> (float x, float y)\n",
    "        node positions\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    pos_communities = _position_communities(g, partition, scale=3.)\n",
    "\n",
    "    pos_nodes = _position_nodes(g, partition, scale=1.)\n",
    "\n",
    "    # combine positions\n",
    "    pos = dict()\n",
    "    for node in g.nodes():\n",
    "        pos[node] = pos_communities[node] + pos_nodes[node]\n",
    "\n",
    "    return pos\n",
    "\n",
    "def _position_communities(g, partition, **kwargs):\n",
    "\n",
    "    # create a weighted graph, in which each node corresponds to a community,\n",
    "    # and each edge weight to the number of edges between communities\n",
    "    between_community_edges = _find_between_community_edges(g, partition)\n",
    "\n",
    "    communities = set(partition.values())\n",
    "    hypergraph = nx.DiGraph()\n",
    "    hypergraph.add_nodes_from(communities)\n",
    "    for (ci, cj), edges in between_community_edges.items():\n",
    "        hypergraph.add_edge(ci, cj, weight=len(edges))\n",
    "\n",
    "    # find layout for communities\n",
    "    pos_communities = nx.spring_layout(hypergraph, **kwargs)\n",
    "\n",
    "    # set node positions to position of community\n",
    "    pos = dict()\n",
    "    for node, community in partition.items():\n",
    "        pos[node] = pos_communities[community]\n",
    "\n",
    "    return pos\n",
    "\n",
    "def _find_between_community_edges(g, partition):\n",
    "\n",
    "    edges = dict()\n",
    "\n",
    "    for (ni, nj) in g.edges():\n",
    "        ci = partition[ni]\n",
    "        cj = partition[nj]\n",
    "\n",
    "        if ci != cj:\n",
    "            try:\n",
    "                edges[(ci, cj)] += [(ni, nj)]\n",
    "            except KeyError:\n",
    "                edges[(ci, cj)] = [(ni, nj)]\n",
    "\n",
    "    return edges\n",
    "\n",
    "def _position_nodes(g, partition, **kwargs):\n",
    "    \"\"\"\n",
    "    Positions nodes within communities.\n",
    "    \"\"\"\n",
    "\n",
    "    communities = dict()\n",
    "    for node, community in partition.items():\n",
    "        try:\n",
    "            communities[community] += [node]\n",
    "        except KeyError:\n",
    "            communities[community] = [node]\n",
    "\n",
    "    pos = dict()\n",
    "    for ci, nodes in communities.items():\n",
    "        subgraph = g.subgraph(nodes)\n",
    "        pos_subgraph = nx.spring_layout(subgraph, **kwargs)\n",
    "        pos.update(pos_subgraph)\n",
    "\n",
    "    return pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to install networkx 2.0 compatible version of python-louvain use:\n",
    "# pip install -U git+https://github.com/taynaud/python-louvain.git@networkx2\n",
    "from community import community_louvain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(list(G['issue'].items())).rename(columns={0:'word', 1:'weight_attr'}) \\\n",
    "    .assign(weight = lambda df: df.weight_attr.map(lambda cell: cell['weight'])) \\\n",
    "    .drop(['weight_attr'], axis=1) \\\n",
    "    .sort_values('weight', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Communities around the word \"problem\"\n",
    "# To save picture, right click on the picture and select \"Save image as...\"\n",
    "matplotlib.rcParams['figure.figsize'] = (40, 40)\n",
    "G=nx.ego_graph(G=graph_of_words, radius=1, n='problem')\n",
    "partition = community_louvain.best_partition(G)\n",
    "pos = community_layout(g=G, partition=partition)\n",
    "nx.draw(G, pos, node_color=list(partition.values()), \n",
    "        labels=dict((n,n) for n,d in G.nodes(data=True)), font_color='black', font_size=8, font_weight='bold',\n",
    "       edge_color='lightgray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Around the word \"issue\"\n",
    "G=nx.ego_graph(G=graph_of_words, radius=1, n='issue')\n",
    "partition = community_louvain.best_partition(G)\n",
    "pos = community_layout(g=G, partition=partition)\n",
    "matplotlib.rcParams['figure.figsize'] = (40, 40)\n",
    "nx.draw(G, pos, node_color=list(partition.values()), \n",
    "        labels=dict((n,n) for n,d in G.nodes(data=True)), font_color='black', font_size=8, font_weight='bold',\n",
    "       edge_color='lightgray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Around the word \"trouble\"\n",
    "G=nx.ego_graph(G=graph_of_words, radius=1, n='trouble')\n",
    "partition = community_louvain.best_partition(G)\n",
    "pos = community_layout(g=G, partition=partition)\n",
    "matplotlib.rcParams['figure.figsize'] = (40, 40)\n",
    "nx.draw(G, pos, node_color=list(partition.values()), \n",
    "        labels=dict((n,n) for n,d in G.nodes(data=True)), font_color='black', font_size=8, font_weight='bold',\n",
    "       edge_color='lightgray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Around the combination of each 3 words\n",
    "G=nx.compose_all([nx.ego_graph(G=graph_of_words, radius=1, n='issue'), \n",
    "                 nx.ego_graph(G=graph_of_words, radius=1, n='problem'),\n",
    "                 nx.ego_graph(G=graph_of_words, radius=1, n='trouble')])\n",
    "partition = community_louvain.best_partition(G)\n",
    "pos = community_layout(g=G, partition=partition)\n",
    "matplotlib.rcParams['figure.figsize'] = (40, 40)\n",
    "nx.draw(G, pos, node_color=list(partition.values()), \n",
    "        labels=dict((n,n) for n,d in G.nodes(data=True)), font_color='black', font_size=8, font_weight='bold',\n",
    "       edge_color='lightgray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This line of code probably does not work, didn't have time to finish this but the idea is to\n",
    "# save the communities (a.k.a topics) and the words in them in a table\n",
    "df = pd.DataFrame([partition]).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentiment analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob import TextBlob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentiment(text):\n",
    "    try:\n",
    "        return TextBlob(text).sentiment.polarity\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "data[\"sentiment\"] = raw_data[\"text\"].map(sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "eleven-classif",
   "language": "python",
   "name": "eleven-classif"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
